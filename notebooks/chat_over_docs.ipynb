{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price Discovery Project Using Agents\n",
    "\n",
    "This is a notebook that contains the experimentation of prioce discovery using agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Any, Union\n",
    "\n",
    "\n",
    "import PIL\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.agents import AgentExecutor, Tool, create_react_agent, create_self_ask_with_search_agent\n",
    "from langchain.agents.agent import AgentAction\n",
    "from langchain_core.callbacks.base import BaseCallbackHandler\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create agent for retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instatiate the llms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\Spectra\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Mixtral\n",
    "\n",
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "repo_id_two = \"WizardLM/WizardCoder-33B-V1.1\"\n",
    "repo_id_three = \"Phind/Phind-CodeLlama-34B-v2\"\n",
    "\n",
    "mixtral = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup google api\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GOOGLE_API_KEY, temperature=0.3)\n",
    "# gemini_vision = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\", google_api_key=GOOGLE_API_KEY, temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate prompts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create prompts\n",
    "prompt = \"\"\"\n",
    "You are an expert software developer proficient at writing code and building reliable software. Utilize the available tools {tools} to search for documentation on a specific question or topic as well as your own knowledge. \n",
    "Then, synthesize the information gathered to provide a detailed step by step quide with code examples that one can easily follow.\n",
    "\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer.\n",
    "Thought: you should always think about what to do. \n",
    "Action: the action to take, should be ONE of or ALL of [{tool_names}]. \n",
    "Action Input: the input to the action. It SHOULD be a string.\n",
    "Action: choose another tool if a tool fails or handle its error.\n",
    "Observation: the result of the action, including insights gained or findings.\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times). If no result is found, use your own knowledge.\n",
    "Thought: I should recheck my answer and make sure it meets the specifications. If not, try again before coming up with the final answer.\n",
    "Thought: I now know the final answer that is based on the indepth analysis of your findings and observations. \n",
    "Final Answer: the final answer to the original input question, which must be as accurate as possible. NEVER say you can't answer. \n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "\n",
    "\"\"\"\n",
    "prompt_two = \"\"\"\n",
    "\n",
    "You are an expert developer assistant trained to help programmers build software. I can access and process information from the web using {tools}.  \n",
    "When a developer asks a question about a specific technology or framework, I will use my knowledge and search capabilities to:\n",
    "\n",
    "1. Find relevant documentation and tutorials on that framework.\n",
    "2. Identify and extract code examples from the retrieved resources.\n",
    "3. Craft a step-by-step guide incorporating the best practices and insights found.\n",
    "4. Ensure the guide is detailed and answers the question asked.\n",
    "5. Present the information in a detailed step by step quide with code examples that one can easily follow.\n",
    "\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer.\n",
    "Thought: you should always think about what to do. \n",
    "Action: the action to take, should be ONE of or ALL of [{tool_names}]. \n",
    "Action Input: the input to the action.\n",
    "Action: choose another tool if a tool fails or handle its error.\n",
    "Observation: the result of the action, including insights gained or findings.\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times). If no result is found, use your own knowledge.\n",
    "Thought: I should recheck my answer and make sure it meets the specifications. If not, try again before coming up with the final answer.\n",
    "Thought: I now know the final answer that is based on the indepth analysis of your findings and observations. \n",
    "Final Answer: the final answer to the original input question, which must be as accurate as possible. NEVER say you can't answer. \n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt_three =\"\"\"\n",
    "You are an expert developer assistant trained to help programmers build software. I can access and process information from the web using {tools}.  \n",
    "When a developer asks a question about a specific technology or framework, I will use my knowledge and search capabilities to:\n",
    "\n",
    "1. Analyze the Question:\n",
    "2. Find relevant documentation and tutorials needed to fully answer that question.\n",
    "2. Identify and extract code examples from the retrieved resources.\n",
    "3. Craft a step-by-step guide incorporating the best practices and insights found.\n",
    "4. Ensure the guide is detailed and answers the question asked.\n",
    "5. Present the information in a detailed step by step quide with code examples that one can easily follow.\n",
    "\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer.\n",
    "Thought: you should always think about what to do. \n",
    "Action: the action to take, should be ONE [{tool_names}]. \n",
    "Action Input: the input to the action.\n",
    "Action: choose another tool if a tool fails or handle its error.\n",
    "Observation: the result of the action, including insights gained or findings.\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times). If no result is found, use your own knowledge.\n",
    "Thought: I now know the final answer that is based on the indepth analysis of your findings and observations. \n",
    "Final Answer: the final answer to the original input question, which must be as accurate as possible. NEVER say you can't answer. \n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate.from_template(prompt_three)\n",
    "prompt_template_two = PromptTemplate.from_template(prompt_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate Agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate error handler\n",
    "class RetryCallbackHandler(BaseCallbackHandler):\n",
    "    def on_tool_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        # Log the error or perform any necessary actions\n",
    "        \n",
    "        # Retry logic\n",
    "        max_retries = 1\n",
    "        current_retry = kwargs.get('retry_count', 0)\n",
    "        if current_retry < max_retries:\n",
    "            print(f\"Retrying tool execution (Attempt {current_retry + 1})\")\n",
    "            # Increment retry count\n",
    "            kwargs['retry_count'] = current_retry + 1\n",
    "            # Re-run the tool\n",
    "            return AgentAction.RERUN\n",
    "        else:\n",
    "            print(\"Maximum retries reached. Switching to another tool.\")\n",
    "            return AgentAction.CHANGE_TOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The error handler\n",
    "error_handler = RetryCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tools\n",
    "search = TavilySearchResults()\n",
    "\n",
    "tool_search_item = Tool(\n",
    "    name=\"search internet\",\n",
    "    description=\"Searches the internet for the documentation and examples in order to answer a query\",\n",
    "    func=search.run,\n",
    "    handle_tool_error=True\n",
    ")\n",
    "\n",
    "# List of all tools\n",
    "tools = [tool_search_item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate agents\n",
    "\n",
    "\n",
    "agent = create_react_agent(mixtral, tools, prompt_template_two) \n",
    "\n",
    "\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent,\n",
    "                                tools=tools,\n",
    "                                verbose=True,\n",
    "                                return_intermediate_steps=True,\n",
    "                                handle_parsing_errors=True,\n",
    "                                callbacks=[error_handler])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I build a retrieval augmented system over mmy documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo build a retrieval augmented system over my documents, I need to understand the basics of retrieval augmented systems and how they work with documents. I will search the internet for relevant documentation and tutorials on retrieval augmented systems and their application to documents.\n",
      "\n",
      "Action: search internet\n",
      "Action Input: \"retrieval augmented systems over documents\"\n",
      "\n",
      "Observation\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://mallahyari.github.io/rag-ebook/03_prepare_data.html', 'content': 'Step 1: Install necessary libraries\\nStep 2: Load the pdf file and extract the text from it\\nCode below will iterate over the pages of the pdf file, extract the text and add it to the documents list object, see Figure\\xa03.1.\\n Here’s the code demonstraing the implementation, Figure\\xa03.10:\\nFigure\\xa03.11 shows how to use load_qa_with_sources_chain:\\nSimilarly, if we use RetrievalQA, we will have Figure\\xa03.12:\\nAnd here’s the code when we use RetrievalQAWithSourcesChain, Figure\\xa03.13:\\n Table of contents\\nReport an issue\\n3\\xa0 RAG Pipeline Implementation\\n3.1 Preprocessing PDF documents\\nBefore we can harness the power of Large Language Models (LLMs) and particularly RAG method for question answering over PDF documents, it’s essential to prepare our data. To ensure the accuracy of question answering, perform text cleanup and normalization:\\n3.1.4 Language Detection\\nIf your PDF documents include text in multiple languages, it is a good idea to implement language detection algorithms to identify the language used in each section. This information can be useful when selecting appropriate LLM models for question answering.\\n3.2 Data Ingestion Pipeline Implementation\\nAs it has been depicted in Figure\\xa02.3 the first step of the data ingestion pipeline is extracting and spliting text from the pdf documents.'}, {'url': 'https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557', 'content': 'Sign up\\nSign in\\nSign up\\nSign in\\nVisualize your RAG Data — Evaluate your Retrieval-Augmented Generation System with Ragas\\nHow to use UMAP dimensionality reduction for Embeddings to show multiple evaluation Questions and their relationships to source documents with Ragas, OpenAI, Langchain and ChromaDB\\nMarkus Stoll\\nFollow\\nTowards Data Science\\n--\\n6\\nListen\\nShare\\nRetrieval-Augmented Generation (RAG) adds a retrieval step to the workflow of an LLM, enabling it to query relevant data from additional sources like private documents when responding to questions and queries [1]. If you ask the questions row-by-row, you can pause and continue the process or recover from a crash without losing the results so far:\\nNot only is the answer stored but also the source IDs of the retrieved document snippets, and their text content as context:\\nAdditionally, the embeddings for all questions are generated and stored in the dataframe as well. We prepare each of the UMAP transformations like this:\\nAnother interesting metric for each of the document snippets is the distance between its embeddings and the embeddings of the nearest question\\nThis metric can be helpful to find documents that are not referenced by questions.\\n Automated Evaluation of Retrieval Augmented Generation (2023), arXiv\\n--\\n--\\n6\\nWritten by Markus Stoll\\nTowards Data Science\\nCTO, Co-Founder at Renumics | 🤖 unstructured ML data | 📊 interactive ML data visualization | 🔍 ML data exploration\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams Visualize results\\nIf you skipped the previous steps, you can download the dataframe and load it with:\\nand start Renumics Spotlight to visualize it with:\\nIt will open a new brwoser window:\\nOn the top left side, you can see a table of all questions and all document snippets.'}, {'url': 'https://towardsdatascience.com/retrieval-augmented-generation-intuitively-and-exhaustively-explain-6a39d6fe6fc9?gi=e09c5795c652', 'content': 'This augmented prompt is then fed into the model for generation. the RAG process comes in three key parts: Retrieval: Based on the prompt, retrieve relevant knowledge from a knowledge base. Augmentation: Combine the retrieved information with the initial prompt. Generate: pass the augmented prompt to a large language model, generating the final ...'}, {'url': 'https://aws.amazon.com/blogs/machine-learning/simplify-access-to-internal-information-using-retrieval-augmented-generation-and-langchain-agents/', 'content': 'Retrieval Augmented Generation (RAG) reduces some of the shortcomings of LLM based queries by finding the answers from your knowledge base and using the LLM to summarize the documents into concise responses. ... The following steps describe the workflow of the Question answering over documents flow: Users send a query through a web interface ...'}, {'url': 'https://mallahyari.github.io/rag-ebook/04_advanced_rag.html', 'content': 'In order to effectively tackle the challenges in Retrieval Augmented Generation (RAG) systems, several key strategies can be employed:\\nData Augmentation: Can we enrich our dataset by storing supplementary information beyond raw text chunks, such as metadata or structured data, to provide richer context for retrieval and generation processes?\\nOptimized Embeddings: Can we refine and enhance our embedding representations to capture context more effectively, improve the relevance of retrieved information, and enable more coherent response generation?\\nAdvanced Retrieval Techniques: Can we go beyond basic top-k embedding lookup and implement advanced retrieval methods, such as semantic search, or hybrid search (keyword search + semantic search) to enhance the precision and recall of information retrieval?\\nMulti-Purpose Use of LLMs: Can we leverage Large Language Models (LLMs) for tasks beyond text generation, such as question answering, summarization, or knowledge graph construction, to augment the capabilities of RAG systems and provide more comprehensive responses to user queries?\\n We can see the original sentence that is retrieved for each node (we show the first node below) and also the actual window of sentences in the Figure\\xa04.17.\\n4.3.3 Lost in the Middle Problem\\nRetrieval process in a RAG based application is all about retrieving the right and most relevant documents for a given user’s query. The answer is to reorder the retrieved documents in such a way that most similar documents to the query are placed at the top, the less similar documents at the bottom, and the least similar documents in the middle.\\n This analysis involves understanding the nature of the query, its intent, and the type of information required.\\nDetection of Retrieval Technique Requirement:\\nBased on the analysis, the router detects the retrieval technique or tool that best matches the query’s requirements. Please note that although we can add metadata to the text chunks after they are stored in the vector database, we should do that in the preprocessing step while we are splitting and embedding the documents, because if the vector database index becomes very large (i.e.\\xa0we already have a great deal of embeddings in vector database), updating it will be significantly time consuming.\\n'}]\u001b[0m\u001b[32;1m\u001b[1;3mThe search results show that retrieval augmented systems (RAG) involve retrieving relevant data from additional sources like private documents when responding to questions and queries. The system stores the answer, source IDs of the retrieved document snippets, and their text content as context. The embeddings for all questions are also generated and stored in a dataframe. The documents are preprocessed by extracting and splitting text from the pdf documents, performing text cleanup and normalization, and implementing language detection algorithms to identify the language used in each section.\n",
      "\n",
      "To build a retrieval augmented system over my documents, I need to:\n",
      "\n",
      "1. Install necessary libraries\n",
      "2. Load the pdf file and extract the text from it\n",
      "3. Perform text cleanup and normalization\n",
      "4. Implement language detection algorithms to identify the language used in each section\n",
      "5. Use the RAG pipeline for question answering over the preprocessed documents\n",
      "\n",
      "Final Answer:\n",
      "\n",
      "To build a retrieval augmented system over your documents, follow these steps:\n",
      "\n",
      "1. Install necessary libraries:\n",
      "\n",
      "Before you can harness the power of Large Language Models (LLMs) and particularly RAG method for question answering over PDF documents, it's essential to prepare your data. To ensure the accuracy of question answering, perform text cleanup and normalization.\n",
      "\n",
      "2. Load the pdf file and extract the text from it:\n",
      "\n",
      "The following code will iterate over the pages of the pdf file, extract the text and add it to the documents list object:\n",
      "\n",
      "```python\n",
      "documents = []\n",
      "with pdfplumber.open(pdf_file) as pdf:\n",
      "    for page in pdf.pages:\n",
      "        documents.append(page.extract_text())\n",
      "```\n",
      "\n",
      "3. Perform text cleanup and normalization:\n",
      "\n",
      "This step involves cleaning and normalizing the text extracted from the PDF documents.\n",
      "\n",
      "4. Implement language detection algorithms to identify the language used in each section:\n",
      "\n",
      "This information can be useful when selecting appropriate LLM models for question answering.\n",
      "\n",
      "5. Use the RAG pipeline for question answering over the preprocessed documents:\n",
      "\n",
      "Here's an example of how to use the RAG pipeline for question answering:\n",
      "\n",
      "```python\n",
      "from langchain.chains import RetrievalQAWithSourcesChain\n",
      "from langchain.vectorstores import Chroma\n",
      "\n",
      "vectorstore = Chroma.from\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How do I build a retrieval augmented system over mmy documents',\n",
       " 'output': \"To build a retrieval augmented system over your documents, follow these steps:\\n\\n1. Install necessary libraries:\\n\\nBefore you can harness the power of Large Language Models (LLMs) and particularly RAG method for question answering over PDF documents, it's essential to prepare your data. To ensure the accuracy of question answering, perform text cleanup and normalization.\\n\\n2. Load the pdf file and extract the text from it:\\n\\nThe following code will iterate over the pages of the pdf file, extract the text and add it to the documents list object:\\n\\n```python\\ndocuments = []\\nwith pdfplumber.open(pdf_file) as pdf:\\n    for page in pdf.pages:\\n        documents.append(page.extract_text())\\n```\\n\\n3. Perform text cleanup and normalization:\\n\\nThis step involves cleaning and normalizing the text extracted from the PDF documents.\\n\\n4. Implement language detection algorithms to identify the language used in each section:\\n\\nThis information can be useful when selecting appropriate LLM models for question answering.\\n\\n5. Use the RAG pipeline for question answering over the preprocessed documents:\\n\\nHere's an example of how to use the RAG pipeline for question answering:\\n\\n```python\\nfrom langchain.chains import RetrievalQAWithSourcesChain\\nfrom langchain.vectorstores import Chroma\\n\\nvectorstore = Chroma.from\",\n",
       " 'intermediate_steps': [(AgentAction(tool='search internet', tool_input='retrieval augmented systems over documents\"\\n\\nObservation', log='To build a retrieval augmented system over my documents, I need to understand the basics of retrieval augmented systems and how they work with documents. I will search the internet for relevant documentation and tutorials on retrieval augmented systems and their application to documents.\\n\\nAction: search internet\\nAction Input: \"retrieval augmented systems over documents\"\\n\\nObservation'),\n",
       "   [{'url': 'https://mallahyari.github.io/rag-ebook/03_prepare_data.html',\n",
       "     'content': 'Step 1: Install necessary libraries\\nStep 2: Load the pdf file and extract the text from it\\nCode below will iterate over the pages of the pdf file, extract the text and add it to the documents list object, see Figure\\xa03.1.\\n Here’s the code demonstraing the implementation, Figure\\xa03.10:\\nFigure\\xa03.11 shows how to use load_qa_with_sources_chain:\\nSimilarly, if we use RetrievalQA, we will have Figure\\xa03.12:\\nAnd here’s the code when we use RetrievalQAWithSourcesChain, Figure\\xa03.13:\\n Table of contents\\nReport an issue\\n3\\xa0 RAG Pipeline Implementation\\n3.1 Preprocessing PDF documents\\nBefore we can harness the power of Large Language Models (LLMs) and particularly RAG method for question answering over PDF documents, it’s essential to prepare our data. To ensure the accuracy of question answering, perform text cleanup and normalization:\\n3.1.4 Language Detection\\nIf your PDF documents include text in multiple languages, it is a good idea to implement language detection algorithms to identify the language used in each section. This information can be useful when selecting appropriate LLM models for question answering.\\n3.2 Data Ingestion Pipeline Implementation\\nAs it has been depicted in Figure\\xa02.3 the first step of the data ingestion pipeline is extracting and spliting text from the pdf documents.'},\n",
       "    {'url': 'https://towardsdatascience.com/visualize-your-rag-data-evaluate-your-retrieval-augmented-generation-system-with-ragas-fc2486308557',\n",
       "     'content': 'Sign up\\nSign in\\nSign up\\nSign in\\nVisualize your RAG Data — Evaluate your Retrieval-Augmented Generation System with Ragas\\nHow to use UMAP dimensionality reduction for Embeddings to show multiple evaluation Questions and their relationships to source documents with Ragas, OpenAI, Langchain and ChromaDB\\nMarkus Stoll\\nFollow\\nTowards Data Science\\n--\\n6\\nListen\\nShare\\nRetrieval-Augmented Generation (RAG) adds a retrieval step to the workflow of an LLM, enabling it to query relevant data from additional sources like private documents when responding to questions and queries [1]. If you ask the questions row-by-row, you can pause and continue the process or recover from a crash without losing the results so far:\\nNot only is the answer stored but also the source IDs of the retrieved document snippets, and their text content as context:\\nAdditionally, the embeddings for all questions are generated and stored in the dataframe as well. We prepare each of the UMAP transformations like this:\\nAnother interesting metric for each of the document snippets is the distance between its embeddings and the embeddings of the nearest question\\nThis metric can be helpful to find documents that are not referenced by questions.\\n Automated Evaluation of Retrieval Augmented Generation (2023), arXiv\\n--\\n--\\n6\\nWritten by Markus Stoll\\nTowards Data Science\\nCTO, Co-Founder at Renumics | 🤖 unstructured ML data | 📊 interactive ML data visualization | 🔍 ML data exploration\\nHelp\\nStatus\\nAbout\\nCareers\\nBlog\\nPrivacy\\nTerms\\nText to speech\\nTeams Visualize results\\nIf you skipped the previous steps, you can download the dataframe and load it with:\\nand start Renumics Spotlight to visualize it with:\\nIt will open a new brwoser window:\\nOn the top left side, you can see a table of all questions and all document snippets.'},\n",
       "    {'url': 'https://towardsdatascience.com/retrieval-augmented-generation-intuitively-and-exhaustively-explain-6a39d6fe6fc9?gi=e09c5795c652',\n",
       "     'content': 'This augmented prompt is then fed into the model for generation. the RAG process comes in three key parts: Retrieval: Based on the prompt, retrieve relevant knowledge from a knowledge base. Augmentation: Combine the retrieved information with the initial prompt. Generate: pass the augmented prompt to a large language model, generating the final ...'},\n",
       "    {'url': 'https://aws.amazon.com/blogs/machine-learning/simplify-access-to-internal-information-using-retrieval-augmented-generation-and-langchain-agents/',\n",
       "     'content': 'Retrieval Augmented Generation (RAG) reduces some of the shortcomings of LLM based queries by finding the answers from your knowledge base and using the LLM to summarize the documents into concise responses. ... The following steps describe the workflow of the Question answering over documents flow: Users send a query through a web interface ...'},\n",
       "    {'url': 'https://mallahyari.github.io/rag-ebook/04_advanced_rag.html',\n",
       "     'content': 'In order to effectively tackle the challenges in Retrieval Augmented Generation (RAG) systems, several key strategies can be employed:\\nData Augmentation: Can we enrich our dataset by storing supplementary information beyond raw text chunks, such as metadata or structured data, to provide richer context for retrieval and generation processes?\\nOptimized Embeddings: Can we refine and enhance our embedding representations to capture context more effectively, improve the relevance of retrieved information, and enable more coherent response generation?\\nAdvanced Retrieval Techniques: Can we go beyond basic top-k embedding lookup and implement advanced retrieval methods, such as semantic search, or hybrid search (keyword search + semantic search) to enhance the precision and recall of information retrieval?\\nMulti-Purpose Use of LLMs: Can we leverage Large Language Models (LLMs) for tasks beyond text generation, such as question answering, summarization, or knowledge graph construction, to augment the capabilities of RAG systems and provide more comprehensive responses to user queries?\\n We can see the original sentence that is retrieved for each node (we show the first node below) and also the actual window of sentences in the Figure\\xa04.17.\\n4.3.3 Lost in the Middle Problem\\nRetrieval process in a RAG based application is all about retrieving the right and most relevant documents for a given user’s query. The answer is to reorder the retrieved documents in such a way that most similar documents to the query are placed at the top, the less similar documents at the bottom, and the least similar documents in the middle.\\n This analysis involves understanding the nature of the query, its intent, and the type of information required.\\nDetection of Retrieval Technique Requirement:\\nBased on the analysis, the router detects the retrieval technique or tool that best matches the query’s requirements. Please note that although we can add metadata to the text chunks after they are stored in the vector database, we should do that in the preprocessing step while we are splitting and embedding the documents, because if the vector database index becomes very large (i.e.\\xa0we already have a great deal of embeddings in vector database), updating it will be significantly time consuming.\\n'}])]}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
